---
layout: default
title:  'Current Mentors and Project Descriptions'
---


<div style="padding-top: 20px;" class="col-md-8">
  <h2 class="featurette-heading">Winter 2020, Mentors and Projects: </h2>
  <p> The following projects are available in Winter 2020. Note that if two projects are listed for the same mentor,
  only one of the projects will run this quarter. Feel free to express interest in multiple projects with the same mentor
on your application, as it will inform us of interest in a particular topic and we can attempt to offer
a similar project in future quarters. </p>
<h4> Peter Gao: Introduction to Gaussian Processes</h4>
<p class="content" style="padding:0px;margin:0px 20px 0px;">
    <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites: </i> None; interest in programming encouraged
</li>
<li style="padding:0px;margin:0px 20px 0px;"> As a concept, the Gaussian distribution, often referred to as the normal distribution or the bell curve, has cemented itself in the public consciousness. But what about its finite dimensional generalization, the multivariate Gaussian? Or its infinite dimensional counterpart, the Gaussian process? This project has two main aims: first, to discuss and explore how Gaussian processes arise in various subfields like machine learning and spatial statistics; and second, to develop notes (or a website) that explain Gaussian processes to a general audience. Of course, the exact focus of this project is flexible, based on the reader's interests/background.
</li>
</p>
  <h4> Kristof Glauninger: Nonparametric Regression </h4>
  <p class="content" style="padding:0px;margin:0px 20px 0px;">
      <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites: </i> Familiarity with linear regression and basic probability, comfort with algebra, some calculus
 </li>
<li style="padding:0px;margin:0px 20px 0px;"> Nonparametric statistical methods
  have seen an explosion in popularity as datasets have increased in size and complexity.
  The goal of this project will be to introduce students who are familiar with parametric regression
  models to a nonparametric setting. We will explore some of the basic theory and applications of these models,
  as well as an interesting case where we can achieve parametric convergence rates in a nonparametric setting.
</li>
  </p>
  <h4>Zhaoqi Li: Statistical Machine Learning and Data Analysis</h4>
  <p class="content" style="padding:0px;margin:0px 20px 0px;">
      <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites</i> Some familiarity of basic statistical background
        and probability theory in the level of STAT 311;
        some knowledge of Maximum Likelihood Estimation is preferred;
        some familiarity of basic programming is preferred;
         an enthusiasm of reading and experimenting is encouraged.  </li>
         <li style="padding:0px;margin:0px 20px 0px;"> We will discuss the relationship between statistics and machine learning, one of the most popular fields in the world,
           and how statistical techniques could be used in the machine learning framework.
            Topics may include classifiers (e.g., Decision Tree, Naive Bayes),
             training (what is training and the relation to likelihood inference), etc.
           The design could range from experimental to theoretical,
           depending on the background of the student. </li>
  </p>
  <h4>Shane Lubold: Applications of Central Limit Theorems </h4>
  <p class="content" style="padding:0px;margin:0px 20px 0px;">
      <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites: </i> Some exposure to linear algebra, the central limit theorem, and coding (such as R, MATLAB, or Python). </li>
      <li style="padding:0px;margin:0px 20px 0px;"> In this project, we will study central limit theorems (CLTs) and their applications in convex geometry and random matrix theory. CLTs are a common tool to understand the behavior of sample means as the sample size gets large. We will first study general conditions on the variables that guarantee a Gaussian limit and will explore how big the sample size must be until our sample averages appear normal. We will then relax these assumptions and discover what the limit becomes (if it exists!) under less restrictive conditions on the random variables. Finally, we will use simulations and theory to explore two exciting applications of CLTs: random matrix theory and convex geometry. First, we will show that the eigenvalues of many types of random matrices satisfy a central limit theorem as the size of the matrix grows. Finally, we will show that the area of convex hulls generated by points drawn from a Gaussian distribution satisfies a central limit theorem! This project will give students an understanding of when and why CLTs exist and how they can be used to answer exciting questions in applied fields. </li>
  </p>
  <h4>Shane Lubold: Random Graphs </h4>
  <p class="content" style="padding:0px;margin:0px 20px 0px;">
      <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites: </i> Some exposure to probability. Some exposure to, or an interest in, graph theory. </li>
      <li style="padding:0px;margin:0px 20px 0px;"> In this project, we will study random graph theory and how the behavior of these graphs change as the size of the graph grows. We will focus primarily on a simple graph model with a number of interesting properties, the Erdös-Rényi model. In this simple model, we generate a graph on n nodes, where each node connects to any other node with probability p(n), which can depend on the graph size n. We will use theory and simulations to derive key properties of this model, such as the distribution of the degree of a vertex or the number of cliques of any size. We will also explore other exciting properties of this model. For example, if the ratio p*n grows at a certain rate as n gets big, then the graph will, for example, exhibit large cliques with probability 1. The proof of these ideas uses only basic statistical ideas. We will prove the conditions that guarantee this behavior and use simulations to explore how large the graphs must be the see this behavior. This project will expose students to the exciting field of random graphs and will give them a good understanding of how simple statistical tools can answer complex questions. </li>
  </p>
  <h4>Bryan Martin: R Package Development </h4>
  <p class="content" style="padding:0px;margin:0px 20px 0px;">
      <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites: </i> Familiarity with R </li>
      <li style="padding:0px;margin:0px 20px 0px;">  Reproducible statistical analysis depends on good software and coding practices. In this project, we will learn how to go from users of R packages to developers of R packages. We will also practice and implement general software developer skills, including documentation, version control, and unit testing. We will learn how to make our code robust, efficient, and user-friendly. Ideally, you will start with an idea of something you are interested in implementing as an R package, whether it be a statistical model, data analysis application, or anything else, though this is not required! </li>
  </p>
  <h4>Bryan Martin: Sparsity and Penalization </h4>
  <p class="content" style="padding:0px;margin:0px 20px 0px;">
      <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites: </i> STAT 311 highly recommended, some knowledge of regression. </li>
      <li style="padding:0px;margin:0px 20px 0px;">  Many modern applications benefit from the principle of less is more. Whether due to practical computation concerns from big data, overfitting concerns from too many parameters, or estimability concerns from a small sample size, statistical models often require sparsity. Sparsity can improve our predictions, help make the patterns we observe in our data more reproducible, and give our model parameters desirable properties.

Often, sparsity is imposed through penalization, where we include a term in our model to enforce that some parameters are set equal to zero. We will learn about some of the statistical theory underlying how penalization works, and how it impacts our model output, both mathematically and computationally. We will also learn about and compare different sparsity schemes, such as lasso, group lasso, elastic net, and more. We will focus on understanding the different settings in which we might be interested in different forms of sparsity and apply these tools to real data.
</li>
  </p>
<h4>Anna Neufeld: Statistical Natural Language Processing </h4>
<p class="content" style="padding:0px;margin:0px 20px 0px;">
    <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites: </i> Proficiency in a programming language. Knowledge of basic
    probability rules at the level of Stat 311.  </li>
    <li style="padding:0px;margin:0px 20px 0px;"> In most statistics classes, the data you work with are numbers. Text documents
      such as books, articles, and speeches provide massive sources of data that can not be analyzed using the tools from your introductory statistics
      courses. We will explore the field of statistical natural language processing and discuss classification and clustering techniques for text data.
      Applications of such techniques include translation, information retrieval, fake news detection, and sentiment analysis.
      After reviewing the literature to get a sense of the general techniques in NLP, we will select a particular
      text dataset and research question and work on an applied project.
 </li>
</p>
<h4>Johnny Paige: Working with Spatial Data</h4>
<p class="content" style="padding:0px;margin:0px 20px 0px;">
    <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites </i> Probability at the level of STAT 340. MATH 308 or some background in linear algebra, and some programming experience (R language strongly recommended). </li>
    <li style="padding:0px;margin:0px 20px 0px;"> Classical methods in statistics such as regression either assume data is independent of each other or their correlation structure is already known.  Yet datasets in climatology, forestry, demography, epidemiology, astronomy, and other areas of study exhibit signs of spatial correlation—nearby observations tend to be more similar. Spatial statistics is the study of such data: how can we “borrow strength” from nearby observations to make predictions at entirely new locations? How can we estimate the correlation structures exhibited by the data, and better determine how underlying natural processes affect our observations? In this project, we will explore how to model data that is correlated in space, and use them to make spatial predictions. The specific project is flexible, depending on your interests and background.
    </li>
</p>
<h4> Michael Pearce: Nonlinear Regression </h4>
<p class="content" style="padding:0px;margin:0px 20px 0px;">
    <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites: </i> A basic knowledge of linear regression and some experience in R</li>
    <li style="padding:0px;margin:0px 20px 0px;"> Simple linear regression models can be easy to implement and interpret,
       but they don't always fit data well!
       For this project, we'll explore regression methods that relax the assumption of linearity.
        These might include (based on the interest and/or experience level of the student)
        polynomial regression, step functions, regression splines, smoothing splines,
         multivariate adaptive regression splines, and generalized additive models.
         Hopefully, we'll even see how to validate such models using cross-validation!
         We will mostly use James et al.'s "An Introduction to Statistical Learning" Chapter 7.
    </li>
</p>
<h4>Anupreet Porwal: Bayesian Linear regression and applications </h4>
<p class="content" style="padding:0px;margin:0px 20px 0px;">
    <li style="padding:0px;margin:0px 20px 0px;"> <i>Prerequisites: </i> Basic knowledge of probability distributions at the level of
      Stat 394 or Stat 340. Knowledge of Linear Algebra is essential for this project. Familiarity with a programming language may be helpful.</li>
      <li style="padding:0px;margin:0px 20px 0px;"> Often when we fit models to practical applications, we have some prior understanding of the context of the problem/field which could potentially be useful to tune our model along with the data. For example, if you are trying to model the reply times of emails from dept. chair to professors, information about the designation of professors (full-time/assistant) can be helpful information.
Bayesian statistics provides a formal way to incorporate our prior beliefs and information into the model and is particularly useful as it accurately helps to quantify the uncertainty in our inferences. In this project, we wish to discuss basics of Bayes theorem, Bayesian version of Linear regression and if time permits, we will learn about probabilistic matrix factorization (Recommendation systems) and apply these techniques to an interesting problem.</li>
</p>

<h4>Sarah Teichman: Networks </h4>
<p class="content" style="padding:0px;margin:0px 20px 0px;">
    <li style="padding:0px;margin:0px 20px 0px;"> <i> Prerequisites: </i> Stat 311. Some R is useful
    but not required. </li>
    <li style="padding:0px;margin:0px 20px 0px;"> Most of the data that you see in STAT 311 are assumed to be independent.
      However, a lot of interesting datasets include information about individual observations
      and the relationships between them. This type of data can be analyzed as networks, in which nodes
      represent individuals and edges represent relationships between them.
       Networks can be used to study interactions between social groups,
        the spread of contagious diseases, biological cycles, etc.
We will use the textbook "Statistical Analysis of Network Data," along with it's companion text
 "Statistical Analysis of Network Data in R" by Eric D. Kolaczyk. We will additionally read one or
  two papers about an application of network analysis and/or analyze a small
  network in R (based on interest of the student). </li>
</p>



</div>
